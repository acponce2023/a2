{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acponce2023/a2/blob/main/filtering_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este cuaderno se encuentra una adaptación del proceso de filtrado para que podáis trabajar con él y ver su funcionamiento más en detalle.\n",
        "\n",
        "Hay algunos ficheros que tendréis que poner en las carpetas que corresponda, por ejemplo todos los ficheros de frecuencias.\n"
      ],
      "metadata": {
        "id": "_r_GRpyEMlXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy lingua-language-detector\n"
      ],
      "metadata": {
        "id": "6lmqkM0gzJcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf freq.tar\n",
        "!tar -xvf extract.tar"
      ],
      "metadata": {
        "id": "Xrh50SbGzzzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spacy download en_core_web_sm\n",
        "!spacy download de_core_news_sm\n",
        "!spacy download fr_core_news_sm\n",
        "!spacy download it_core_news_sm\n",
        "!spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "wQnMIGfd0dka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import traceback\n",
        "import uuid\n",
        "import os\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "import lingua\n",
        "\n",
        "# Spacy\n",
        "\n",
        "spacy_models = json.loads('''{\n",
        "\t\"bul\": {\"sm\": null, \"lg\": null},\n",
        "\t\"hrv\": {\"sm\": \"hr_core_news_sm\", \"lg\": \"hr_core_news_lg\"},\n",
        "\t\"cze\": {\"sm\": null, \"lg\": null},\n",
        "\t\"dan\": {\"sm\": \"da_core_news_sm\", \"lg\": \"da_core_news_trf\"},\n",
        "\t\"dut\": {\"sm\": \"nl_core_news_sm\", \"lg\": \"nl_core_news_lg\"},\n",
        "\t\"eng\": {\"sm\": \"en_core_web_sm\", \"lg\": \"en_core_web_trf\"},\n",
        "\t\"est\": {\"sm\": null, \"lg\": null},\n",
        "\t\"fin\": {\"sm\": \"fi_core_news_sm\", \"lg\": \"fi_core_news_lg\"},\n",
        "\t\"fre\": {\"sm\": \"fr_core_news_sm\", \"lg\": \"fr_dep_news_trf\"},\n",
        "\t\"ger\": {\"sm\": \"de_core_news_sm\", \"lg\": \"de_dep_news_trf\"},\n",
        "\t\"gre\": {\"sm\": \"el_core_news_sm\", \"lg\": \"el_core_news_lg\"},\n",
        "\t\"hun\": {\"sm\": null, \"lg\": null},\n",
        "\t\"gle\": {\"sm\": null, \"lg\": null},\n",
        "\t\"ita\": {\"sm\": \"it_core_news_sm\", \"lg\": \"it_core_news_lg\"},\n",
        "\t\"lav\": {\"sm\": null, \"lg\": null},\n",
        "\t\"lit\": {\"sm\": \"lt_core_news_sm\", \"lg\": \"lt_core_news_lg\"},\n",
        "\t\"mlt\": {\"sm\": null, \"lg\": null},\n",
        "\t\"pol\": {\"sm\": \"pl_core_news_sm\", \"lg\": \"pl_core_news_lg\"},\n",
        "\t\"por\": {\"sm\": \"pt_core_news_sm\", \"lg\": \"pt_core_news_lg\"},\n",
        "\t\"rum\": {\"sm\": \"ro_core_news_sm\", \"lg\": \"ro_core_news_lg\"},\n",
        "\t\"rus\": {\"sm\": \"ru_core_news_sm\", \"lg\": \"ru_core_news_lg\"},\n",
        "\t\"slo\": {\"sm\": null, \"lg\": null},\n",
        "\t\"slv\": {\"sm\": null, \"lg\": null},\n",
        "\t\"spa\": {\"sm\": \"es_core_news_sm\", \"lg\": \"es_dep_news_trf\"},\n",
        "\t\"swe\": {\"sm\": null, \"lg\": null}\n",
        "}''')\n",
        "\n",
        "\n",
        "\n",
        "freq_list = {\"spa\": {}, \"ger\": {}, \"fre\": {}, \"eng\": {}, \"ita\": {}}\n",
        "\n",
        "freq_list[\"spa\"][\"1-gram\"] = open(\"./freq/spa/spa-1gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"ger\"][\"1-gram\"] = open(\"./freq/ger/ger-1gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"fre\"][\"1-gram\"] = open(\"./freq/fre/fre-1gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"eng\"][\"1-gram\"] = open(\"./freq/eng/eng-1gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"ita\"][\"1-gram\"] = open(\"./freq/ita/ita-1gram.txt\").read().split(\"\\n\")\n",
        "\n",
        "freq_list[\"spa\"][\"2-gram\"] = open(\"./freq/spa/spa-2gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"ger\"][\"2-gram\"] = open(\"./freq/ger/ger-2gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"fre\"][\"2-gram\"] = open(\"./freq/fre/fre-2gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"eng\"][\"2-gram\"] = open(\"./freq/eng/eng-2gram.txt\").read().split(\"\\n\")\n",
        "freq_list[\"ita\"][\"2-gram\"] = open(\"./freq/ita/ita-2gram.txt\").read().split(\"\\n\")\n",
        "\n",
        "lingua_langs = {\n",
        "\t\t\"eng\": lingua.Language.ENGLISH,\n",
        "\t\t\"spa\": lingua.Language.SPANISH,\n",
        "\t\t\"ita\": lingua.Language.ITALIAN,\n",
        "\t\t\"ger\": lingua.Language.GERMAN,\n",
        "\t\t\"fre\": lingua.Language.FRENCH\n",
        "\t}\n",
        "\n",
        "langs_used = [v for k,v in lingua_langs.items()]\n",
        "\n",
        "lang_detector = lingua.LanguageDetectorBuilder.from_languages(*langs_used).build()"
      ],
      "metadata": {
        "id": "ZQpRAnvHB5cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiEWgGzSMXAf"
      },
      "outputs": [],
      "source": [
        "def filter_terms(lines, lang):\n",
        "\n",
        "\tterms = {}\n",
        "\n",
        "\tfilter_deep_1g = 50000\n",
        "\tfilter_deep_2g = 1000000\n",
        "\n",
        "\tdict_1g = {}\n",
        "\tdict_2g = {}\n",
        "\n",
        "\n",
        "\tif (lang in freq_list) and (\"1-gram\" in freq_list[lang]):\n",
        "\n",
        "\t\tlower_list = [t.lower() for t in freq_list[lang][\"1-gram\"][:filter_deep_1g]]\n",
        "\n",
        "\t\tdict_1g = dict(zip(lower_list, range(len(lower_list))))\n",
        "\n",
        "\tif (lang in freq_list) and (\"2-gram\" in freq_list[lang]):\n",
        "\n",
        "\t\tlower_list = [t.lower() for t in freq_list[lang][\"2-gram\"][:filter_deep_2g]]\n",
        "\n",
        "\t\tdict_2g = dict(zip(lower_list, range(len(lower_list))))\n",
        "\n",
        "\n",
        "\tfor term in lines:\n",
        "\n",
        "\t\tfreq, term = term.replace(\"\\n\", \"\").split(\"\\t\")\n",
        "\n",
        "\t\tterm = term.replace(\"-\", \" \").replace(\"  \", \" \")\n",
        "\n",
        "\t\tif (lang in freq_list) and (\"1-gram\" in freq_list[lang]) and (term.lower() in dict_1g):\n",
        "\n",
        "\t\t\tprint(\"Excluding\", term, \"(too freq 1-gram)\")\n",
        "\n",
        "\n",
        "\t\telif (lang in freq_list) and (\"2-gram\" in freq_list[lang]) and (term.lower() in dict_2g):\n",
        "\n",
        "\t\t\tprint(\"Excluding\", term, \"(too freq 2-gram)\")\n",
        "\n",
        "\n",
        "\t\telif any(len(word) < 4 for word in term.split(\" \")):\n",
        "\n",
        "\t\t\tprint(\"Excluding\", term, \"(too short)\")\n",
        "\n",
        "\n",
        "\t\telif not term.replace(\" \", \"\").replace(\"'\", \"\").replace(\"-\",\"\").isalpha() or term.replace(\" \", \"\").startswith(\"-\") or term.replace(\" \", \"\").endswith(\"-\"):\n",
        "\n",
        "\t\t\tprint(\"Excluding\", term, \"(strange symbols)\")\n",
        "\n",
        "\t\telse:\n",
        "\n",
        "\t\t\tprint(\"Adding\", term)\n",
        "\n",
        "\t\t\tterms[term] = {\"f\": freq}\n",
        "\n",
        "\n",
        "\t# Las diferencias de capitalizacion se resuelven optando por la version mas habitual\n",
        "\n",
        "\tfor term, obj in terms.copy().items():\n",
        "\n",
        "\t\tif term.lower() != term and term.lower() in terms:\n",
        "\n",
        "\t\t\tif int(terms[term.lower()][\"f\"]) > int(terms[term][\"f\"]):\n",
        "\n",
        "\t\t\t\tterms.pop(term)\n",
        "\n",
        "\t\t\t\tprint(\"Excluding\", term, \"(duplicated and less frequent capitalization)\")\n",
        "\n",
        "\t\t\telse:\n",
        "\n",
        "\t\t\t\tterms.pop(term.lower())\n",
        "\n",
        "\t\t\t\tprint(\"Excluding\", term.lower(), \"(duplicated and less frequent capitalization)\")\n",
        "\n",
        "\n",
        "\tvalid_NE = [\"EVENT\", \"FAC\", \"ORG\", \"WORK_OF_ART\"]\n",
        "\n",
        "\tpipe = spacy.load(spacy_models[lang][\"sm\"])\n",
        "\n",
        "\tfor term, obj in terms.copy().items():\n",
        "\n",
        "\t\tdoc = pipe(term)\n",
        "\n",
        "\t\tfor token in doc.ents:\n",
        "\n",
        "\t\t\tprint(\"Found NE: \", token.text, token.label_)\n",
        "\n",
        "\t\t\tif not (token.label_ in valid_NE) and term in terms:\n",
        "\n",
        "\t\t\t\tterms.pop(term)\n",
        "\n",
        "\n",
        "\tfor term, obj in terms.copy().items():\n",
        "\n",
        "\t\tdetected = lang_detector.detect_language_of(term)\n",
        "\n",
        "\t\tprint(term, detected)\n",
        "\n",
        "\t\tif lang in lingua_langs and detected != lingua_langs[lang]:\n",
        "\n",
        "\t\t\tterms.pop(term)\n",
        "\n",
        "\n",
        "\treturn terms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_terms(terms, lang):\n",
        "\n",
        "\tlemmatized_terms = {}\n",
        "\n",
        "\tpipe = spacy.load(spacy_models[lang][\"sm\"])\n",
        "\n",
        "\tterm_list_old = list(terms.keys())\n",
        "\n",
        "\tfor term in term_list_old:\n",
        "\n",
        "\t\tdoc = pipe(term)\n",
        "\n",
        "\t\tfull_token = []\n",
        "\n",
        "\t\tfor token in doc:\n",
        "\n",
        "\t\t\tfull_token.append(token.lemma_)\n",
        "\n",
        "\t\tlemma = \" \".join(full_token)\n",
        "\n",
        "\t\tif term in terms:\n",
        "\n",
        "\t\t\told_f = terms[term]\n",
        "\n",
        "\t\t\tif lemma in lemmatized_terms:\n",
        "\n",
        "\t\t\t\tcurrent_f = lemmatized_terms[lemma]\n",
        "\n",
        "\t\t\t\tnew_f = current_f[\"f\"] + old_f[\"f\"] # Varias palabras convergen en una raíz\n",
        "\n",
        "\t\t\t\tlemmatized_terms[lemma] = {\"f\": new_f}\n",
        "\n",
        "\t\t\telse:\n",
        "\n",
        "\t\t\t\tlemmatized_terms[lemma] = {\"f\": old_f[\"f\"]}\n",
        "\n",
        "\t# Se reaplica filtrado a las palabras luego de filtrarlas, esto estaría\n",
        "  # mejor hacerlo de otra forma, hay código repetido\n",
        "\n",
        "\tfilter_deep_1g = 25000\n",
        "\tfilter_deep_2g = 1000000\n",
        "\n",
        "\tdict_1g = {}\n",
        "\tdict_2g = {}\n",
        "\n",
        "\tif (lang in freq_list) and (\"1-gram\" in freq_list[lang]):\n",
        "\n",
        "\t\tlower_list = [t.lower() for t in freq_list[lang][\"1-gram\"][:filter_deep_1g]]\n",
        "\n",
        "\t\tdict_1g = dict(zip(lower_list, range(len(lower_list))))\n",
        "\n",
        "\tif (lang in freq_list) and (\"2-gram\" in freq_list[lang]):\n",
        "\n",
        "\t\tlower_list = [t.lower() for t in freq_list[lang][\"2-gram\"][:filter_deep_2g]]\n",
        "\n",
        "\t\tdict_2g = dict(zip(lower_list, range(len(lower_list))))\n",
        "\n",
        "\n",
        "\tfor term in lemmatized_terms.copy().keys():\n",
        "\n",
        "    # Solo se extá empleando en \"eng\" porque la lematización en otras cambia\n",
        "    # también otras flexiones y es algo a evitar.\n",
        "\n",
        "\t\tif lang == \"eng\":\n",
        "\n",
        "\t\t\tif (term.lower() in dict_1g):\n",
        "\n",
        "\t\t\t\tprint(\"Excluding\", term, \"(too freq 1-gram) lemma\")\n",
        "\n",
        "\t\t\t\tlemmatized_terms.pop(term)\n",
        "\n",
        "\t\t\telif (term.lower() in dict_2g):\n",
        "\n",
        "\t\t\t\tprint(\"Excluding\", term, \"(too freq 2-gram) lemma\")\n",
        "\n",
        "\t\t\t\tlemmatized_terms.pop(term)\n",
        "\n",
        "\treturn lemmatized_terms"
      ],
      "metadata": {
        "id": "5FH0edJrB073"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = open(\"./extract/eng/occupational_therapy/terms.txt\").readlines()\n",
        "\n",
        "terms = filter_terms(example, \"eng\")\n",
        "\n",
        "print(terms)\n",
        "\n",
        "terms = lemmatize_terms(terms, \"eng\")\n",
        "\n",
        "print(terms)"
      ],
      "metadata": {
        "id": "eL4CxW6R04k8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}